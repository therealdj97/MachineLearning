# notes on ml algo

## pca

1. can be used for facial recognition
2. It is a linear transformmation
3. most popular feature extraqction method
4. consisits of transformation from space of hign dim to another with more reduced dim
5. If data is highly corelated there is redundant information
6. so pca decreases amount of redundant information by decorrelating input vectors
7. input vectors with high dimentions & correlated can be represented in lower dimention space & decorrelated
8. PCA is powerful tool to compress data
9. Principal component analysis (PCA) is a technique used to reduce the dimensionality of a data set while retaining as much of the variation present in the data as possible. It does this by transforming the data to a new coordinate system such that the first axis (called the first principal component) explains the greatest amount of variance in the data, the second axis (second principal component) explains the second greatest amount of variance, and so on. These new axes are called principal components, and the data can be projected onto them to create a lower-dimensional representation of the data. PCA is often used as a tool in exploratory data analysis and for making data easy to visualize.

## Linear Discriminant analysis

1. Linear discriminant analysis (LDA) is a technique used to find a linear combination of features that separates different classes of a data set as well as possible. It is a supervised technique, meaning it requires labeled data. It is commonly used as a dimensionality reduction technique and a classifier. LDA is a method used for finding a linear combination of features that separates different classes of a data set as well as possible.

The goal of LDA is to project the original features onto a new feature space with a lower dimensionality while retaining as much of the class discrimination information as possible. LDA finds the linear combination of features that maximizes the ratio of the between-class variance to the within-class variance. In other words, it finds the feature space that separates the classes as much as possible while keeping the data points within each class as close together as possible.
2. LDA works better than pca when training data is well representative of data in system
3. If data isnt presentative enough pca works better

## INdependent component analysis

## NMF

## AUC ROC CURVE with k folds

## KNN

## Kmeans

## Hierarchical clustering

## Decision trees

### Information Gain

1. ID3
2. c4.5
3. C5
4. J48

### Gini Index

1. SPRINT
2. SLIQ

## Association rule

## Apriori

## xgboost

## YOLO-v3

## Auto encoders

## Nural Networks

1. Convolution nural networks
2. Recurrent nural networks
3. Artificial Nural Networks
4. Layer Nural network

## NLP

1. NLTK
2. BERT
3. SPACY

### Propogation

1. Backward Propogation
2. forward propogation
3. Optimization Algorithm
4. weight & bias

## Regression

1. Logistic regression
2. Linear Regresssion
